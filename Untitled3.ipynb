{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVh5AoWbq7iZJscoByzMIv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manjunatha-kv/Software-Development/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MUWmR3LhK4IL"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import sqlite3\n",
        "from datetime import datetime, date\n",
        "\n",
        "class VergeScraper:\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "        self.articles = []\n",
        "        self.conn = sqlite3.connect('verge_articles.db')\n",
        "        self.cursor = self.conn.cursor()\n",
        "        self.cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS articles (\n",
        "                id INTEGER PRIMARY KEY,\n",
        "                url TEXT,\n",
        "                headline TEXT,\n",
        "                author TEXT,\n",
        "                date TEXT\n",
        "            )\n",
        "        ''')\n",
        "    \n",
        "    def scrape(self):\n",
        "        # Send HTTP request to the website\n",
        "        response = requests.get(self.url)\n",
        "\n",
        "        # Parse the HTML content using BeautifulSoup\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        article_elements = soup.find_all('div', class_='c-entry-box--compact__body')\n",
        "\n",
        "        # Extract information from each article\n",
        "        for idx, article in enumerate(article_elements):\n",
        "            headline = article.h2.text.strip()\n",
        "            url = article.h2.a['href']\n",
        "            author = article.find('span', class_='c-byline__item').text.strip()\n",
        "            date = article.find('span', class_='c-byline__item').next_sibling.strip()\n",
        "\n",
        "            # Convert date string to datetime object\n",
        "            date_obj = datetime.strptime(date, '%B %d, %Y')\n",
        "\n",
        "            # Append article to the list\n",
        "            self.articles.append({\n",
        "                'id': idx,\n",
        "                'url': url,\n",
        "                'headline': headline,\n",
        "                'author': author,\n",
        "                'date': date_obj.date()\n",
        "            })\n",
        "    \n",
        "    def save_csv(self):\n",
        "        # Save articles to CSV file\n",
        "        filename = datetime.now().strftime('%d%m%Y') + '_verge.csv'\n",
        "        with open(filename, 'w', newline='') as csvfile:\n",
        "            fieldnames = ['id', 'url', 'headline', 'author', 'date']\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "            for article in self.articles:\n",
        "                writer.writerow(article)\n",
        "    \n",
        "    def save_to_database(self):\n",
        "        # Save articles to SQLite database\n",
        "        for article in self.articles:\n",
        "            self.cursor.execute('SELECT * FROM articles WHERE url=?', (article['url'],))\n",
        "            result = self.cursor.fetchone()\n",
        "            if result is None:\n",
        "                self.cursor.execute('INSERT INTO articles (id, url, headline, author, date) VALUES (?, ?, ?, ?, ?)',\n",
        "                    (article['id'], article['url'], article['headline'], article['author'], article['date']))\n",
        "        self.conn.commit()\n",
        "    \n",
        "    def run(self):\n",
        "        self.scrape()\n",
        "        self.save_csv()\n",
        "        self.save_to_database()\n",
        "        self.conn.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    scraper = VergeScraper('https://www.theverge.com/')\n",
        "    scraper.run()\n"
      ]
    }
  ]
}